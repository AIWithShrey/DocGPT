{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Pinecone, OpenSearchVectorSearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def get_credentials(secret_id: str, region_name: str) -> str:\n",
    "    \n",
    "    client = boto3.client('secretsmanager', region_name=region_name)\n",
    "    response = client.get_secret_value(SecretId=secret_id)\n",
    "    secrets_value = json.loads(response['SecretString'])    \n",
    "    \n",
    "    return secrets_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creds = get_credentials(\"VectorDB\", \"us-east-1\")\n",
    "http_auth = (\"Shreyas\", \"BigDix@123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('scraped_text_wikipedia.txt') as f:\n",
    "    documents = f.readlines()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100,\n",
    "                                               chunk_overlap  = 20,)\n",
    "chunks = text_splitter.create_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
    }
   ],
   "source": [
    "db = OpenSearchVectorSearch.from_documents(chunks,\n",
    "                                           embeddings,\n",
    "                                            opensearch_url=\"https://search-mynewdomain-7tarbatslpxmi5w3bicukjnlhy.us-east-1.es.amazonaws.com\",\n",
    "                                            http_auth=http_auth,\n",
    "                                            timeout = 300,\n",
    "                                            use_ssl = True,\n",
    "                                            verify_certs = True,\n",
    "                                            index_name=\"webscraping-wikipedia\",\n",
    "                                            engine=\"faiss\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=\"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\",\n",
    "                    temperature=0.3,)\n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 10})\n",
    "chain = RetrievalQA.from_chain_type(llm, chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To fix this error on AWS, you need to increase the value of the [bulk_size] parameter. The error message suggests that the current value of [bulk_size] is set to 500, which is smaller than the number of embeddings (83747) you are trying to process.\n",
      "\n",
      "To increase the [bulk_size] parameter, you will need to modify the configuration or settings of the specific AWS service or application you are using. Without more specific information about the AWS service or application you are working with, it is difficult to provide detailed instructions.\n",
      "\n",
      "I recommend referring to the documentation or support resources for the specific AWS service or application you are using. They should provide guidance on how to adjust the [bulk_size] parameter to a value that can handle the number of embeddings you are working with.\n"
     ]
    }
   ],
   "source": [
    "query = \"RuntimeError: The embeddings count, 83747 is more than the [bulk_size], 500. Increase the value of [bulk_size]. Given this error, how do I fix it on AWS?\"\n",
    "answer = chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
